From -3551656348510303256
X-Google-Language: ENGLISH,ASCII-7-bit
X-Google-Thread: fb739,7449e7356ea20b79
X-Google-Attributes: gidfb739,public
Path: controlnews3.google.com!news1.google.com!newsfeed.stanford.edu!newsfeed.news.ucla.edu!ucdavis!vidi.ucdavis.edu!ez064842
From: ez064842@vidi.ucdavis.edu (Remington Stone)
Newsgroups: alt.games.everquest
Subject: Re: Video Problem... Completely Frustrated...
Date: Mon, 10 May 2004 20:48:07 +0000 (UTC)
Organization: Infoholics Anonymous
Lines: 113
Message-ID: <c7opq7$1dt$1@woodrow.ucdavis.edu>
References: <q_WdnV7fyMu50wHdRVn-hA@comcast.com> <409e547a@duster.adelaide.on.net> <btBnc.406613$Pk3.148487@pd7tw1no> <409ef115$1@duster.adelaide.on.net>
NNTP-Posting-Host: vidi.ucdavis.edu
X-Trace: woodrow.ucdavis.edu 1084222087 1469 169.237.105.39 (10 May 2004 20:48:07 GMT)
X-Complaints-To: usenet@ucdavis.edu
NNTP-Posting-Date: Mon, 10 May 2004 20:48:07 +0000 (UTC)
Xref: controlnews3.google.com alt.games.everquest:1623

@ndrew said:
}42 wrote:
}> I find spell effects in a busy (even nonraid) fight can even obsure
}> critcal items ... like incoming adds.
}> But overall that's wonderful, and for those of us with the money to
}> burn, by all means... But for the average person, or even my 2nd
}> computer, which i barely look at ... I don't need any of that.
}At a cost of just over $US200 for a very very good card it is a no
}brainer for those wishing to upgrade for graphics that are DX9
}compatible.

Given my budget, a $200 investment is in no way a no brainer.  That's the 
high end of 'feasible'.

}> Top cards with a top system. Absolutely. Average cards in average
}> systems... you have to to make choices. If everyone had top cards in
}> top systems then this would be a nonissue.
}The graphics business revolves around gamers in the main, this is where
}there money is made.  I would suggest to you that most hardcore gamers
}upgrade their systems each 2 years at most. In the case of the Nvidia
}4200 and the rest of the GeForce 4 series now is the time.

Yea, I think most of my system is about 2 years old now.  My GF3 is 
probably the newest component.  Oh, except the power supply, which is 
already acting like a disappointing POS.

}> Not at all. 26FPS is playable. 
}Disagree with this ... I don't know anyone who willingly put up with
}bad performance like this.

*Shrug*  I would.  Consistent 26FPS+ would be plenty in my book.

}> If I've eye candy turned on and im
}> getting 26fps then I really ought turn the eye candy off. But if I'm
}> running bare bones settings, and I'm getting 26fps then the game is
}> playable. Its not optimal, and for a game like eq in particular...
}> its adequate. If my 2nd box gets me 26fps that's plenty.
}As long as you don't want to play games with it suffering the lag and
}lack of visual effects that is fine.  If you do play games with it and
}wish to have a substandard system that is also fine.

I actually hate most of the eye candy.  I turn grass off, I usually have 
all spells off, I frequently have the sky off, my clip plane never goes 
above 40%.  Sometimes I turn on just my group's spell effects (thank 
goodness I can do that now!  Woot!) But if the mob aoe's or there's a bard 
within 3 parsecs, it gets turned straight back off.  Sure, perhaps the 
artist intended for me to see all that crap.  Too bad.  I want to see the 
adds, I want to watch the mob squirm around, I want to see where the 
shrunken MT is standing.  I -don't- want to see a huge sheet of aoe flame 
or a swirling cloud of bard song sparklies.  And I don't want to see a 
sheet of pouring rain, no matter how artistically rendered it is.  Thank 
god for Wake of Karana!

}> 26fps sustained fps in eq in particular is not too slow. It is the
}> edge of playable, but its fine if that's your nominal fps. IF that's
}> your maximum fps, and it drops down to 10 when the scene gets busy
}> then yes its not playable, but that's not what I've ever suggested is.
}Hands up those who want to play a first person shooter at 26fps .. come
}on.

Heh, I don't play FP shooters, I play EQ. :)

}> That's great.
}> But that doesn't change the fact that the average persons screen is
}> set to a refresh rate of 60Hz. 
}Oh rubbish ... I haven't had a screen set to 60Mhz for years and I
}seriously doubt any of the players in this forum have their screens set
}to that.I would just love to see some proof on this one.

I do.  What proof would you like, my dxdiags? :) I generally leave it 
set at default in Windows, as I've had some very bad experiences trying to 
change that value.  My ancient (but actually pretty good) monitor defaults 
to 60Hz.  It's starting to make a horrible humming noise though, I may 
need to replace or service that bugger soon.  Most of the monitors I've 
worked with have defaulted to 60, 70, or 72Hz.  I'd say at least 40% were 
in the 60Hz camp...  I haven't done any sort of scientific study, but I'd 
say 70/72Hz was more common for early-90's monitors, 60Hz was more common 
in late 90's ones, and this decades' seem evenly split between the two.

This does beg a more interesting (to me) question.  Waay back in high 
school, I took an animation class.  This being 1986, we used actual 
cameras and cels and bits of paper and stuff, rather than the modern 
computer approach.  I remember the statistic being cited that 16-18 fps 
was the point at which our brains would stop interpreting what we saw as a 
series of pictures and start seeing it as animation.  So clearly, less 
than 18fps is entirely unacceptable.

Then, I moved on, and ended up studying a fair amount of biochemistry.  
Including, at some point, studying the chemical basis of how vision 
worked, with all the rods and cones and cyclic AMP and GMP and stuff that 
goes on when we actually see stuff.  And I remember actually working out 
at some point -roughly- what the refresh rate of a healthy human retina 
was.  It was only an order-of-magnitude calculation done on a scrap of 
notepaper I'd never manage to find again, but it came out in the ballpark 
of 10-100FPS. (It related to the rate at which one of those signal 
molecules, cGMP probably, could reach equilibrium for the known 
concentrations the enzymes that processed it.)  So I'm thinking that 
monitor manufacturers have surely had someone do this calculation at some 
point, probably much more carefully than I did, and surely, they've 
empirically tested what frequencies work and how they look, and didn't 
choose 60Hz as the lowest default by accident.  They probably worked out 
the average human limits of perception and added a safety margin.  On a 
monitor running well at 60Hz, I don't see flicker.  Of course, if it's 
running poorly, it doesn't matter what the refresh rate is. :)

But I strongly suspect that the limiting factor on useful FPS has nothing 
to do with the refresh rate.  Your eyes just aren't infinitely quick, and 
you can't make the chemistry happen any faster.  I don't know, despite 
having eyeballed it myself, exactly how high that limit is, though.  
Anyone know of a more indepth paper on the subject? :)  Maybe if I get a 
bit more bored, I'll go try doing that calculation again.

[65 Coercer] Zinphandel Chianti <Prism> (Gnome) Ayonae Ro



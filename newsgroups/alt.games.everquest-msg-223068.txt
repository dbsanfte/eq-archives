From 2435092091853749541
X-Google-Language: ENGLISH,ASCII-7-bit
X-Google-Thread: fb739,8799123873439500
X-Google-Attributes: gidfb739,public
X-Google-ArrivalTime: 2003-11-06 15:51:42 PST
Path: archiver1.google.com!news1.google.com!newsfeed.stanford.edu!newsfeed.news.ucla.edu!ihnp4.ucsd.edu!ucdavis!vici.ucdavis.edu!ez064842
From: ez064842@vici.ucdavis.edu (Remington Stone)
Newsgroups: alt.games.everquest
Subject: Re: OT: Morality
Date: Thu, 6 Nov 2003 23:51:20 +0000 (UTC)
Organization: Infoholics Anonymous
Lines: 87
Message-ID: <boempo$hq6$1@woodrow.ucdavis.edu>
References: <3FA282F0.9DEED111@dejazzd.com> <wqHpb.285870$6C4.211095@pd7tw1no> <bocasr$4m$1@woodrow.ucdavis.edu> <JKiqb.313766$pl3.272186@pd7tw3no>
NNTP-Posting-Host: vici.ucdavis.edu
X-Trace: woodrow.ucdavis.edu 1068162680 18246 169.237.105.41 (6 Nov 2003 23:51:20 GMT)
X-Complaints-To: usenet@ucdavis.edu
NNTP-Posting-Date: Thu, 6 Nov 2003 23:51:20 +0000 (UTC)
Xref: archiver1.google.com alt.games.everquest:6266

42  said:
}> Hmm, I think you've identified the difference between the Christian 
}> version and Kant's version.  The difference is the Christian version says, 
}> "You should kill others if you want them to kill you".  Kant's version 
}> says "You should kill others if you want them to kill you and you don't 
}> care what happens to everyone else".  Still, in either case, it's possible 
}> for someone to be psycho enough that the statement is exhorting them to 
}> random murder.
}Nobody ever asserted that we should be calibrating our moral compass 
}with psychotics. Its almost self-evident that any moral guidelines that 
}you derive from what insane sociopaths think is going to be pretty 
}questionable.

I'm not suggesting that we derive moral guidelines from how psychos think.  
I'm suggesting that a moral guideline that doesn't take into account that 
we don't all think the same is not perfect.  I will not assert that we 
should calibrate our moral compass -with- psychotics, but that a moral 
compass that is not calibrated so that even psychos can use it is less 
than ideal.

Statistics show that one in four folks will be diagnosed with some sort of 
mental disorder in their lifetime, don't they?  How good is a Golden Rule 
that only works for 3 people in 4?  Sure, the other 25% may not be crazy 
in that exactway, but there is probably some point (and probably a less 
dramatic one) where their desires will differ from the desires of those 
their actions affect.  Which is all that is required for the rule to go a 
bit haywire, isn't it?

}> }> It's a funny old world, isn't it?  I do like the axiom, but it's not 
}> }> -quite- perfect, somehow.
}> }The maxim presupposes a generous helping of self-interest. It is not in 
}> }anybodies self interest to wipe everyone out.
}> No, it's not really self-interest.  If you're suicidal and too chicken to 
}> do it yourself, it's arguably in your self-interest to convnce others to 
}> kill you.  
}Terrible example. That is a far cry from thinking that everybody should 
}be killed. Your just making some sort of right to suicide argument, 
}which is far far less controversial than advocating mass slaughter on 
}all sides.

No, that's not quite what I'm doing.  I'm suggesting that there are those 
for whom the Golden Rule would be an incitement to mass slaughter rather 
than a prohibition against it.  And in this particular part, I'm debating 
your use of the term self-interest as opposed to self-preservation, which 
seems closer to what you probably meant.

The neat thing is, the original example I gave also works backward.  
Assume for a moment you're a doctor.  A doctor who believes very, very, 
strongly that life should be preserved in all instances.  Perhaps a doctor 
who believes that there is no afterlife or reincarnation or -anything- 
after death.  In any case, one who believes that in any instance where 
life can be prolonged, it should be, which is, of course, a universal 
rule, in keeping with Kant's version.  Now assume you have a patient in a 
coma who will (probably) never recover, and who may well be suffering 
greatly in this coma, you have no way of knowing.  And assume the 
patient has left a living will saying he wants to be unplugged should 
those circumstances ever occur.  The Golden Rule says very clearly that 
you should not unplug this patient, since that's what -you- would want, 
isn't it?  But is that morally right, given the patient's express wish?

It needn't be a matter of life or death.  I only went with a matter 
of life or death because someone a few posts back said something 
like "well, it clearly follows that everyone would agree this rules out 
random murder."  A statement that only needed a one-psycho example to 
disprove, in any case. :)

Assume person A likes being patted on the head, and sees no reason anyone 
else would mind it, and person B doesn't like it because it reminds him of 
his annoying uncle.  The rule tells person A to pat person B on the head 
anyway, no matter how person B feels about it.  At least until person B 
slugs him in the arm and explains why. :)

Assume person A would always want help with a mob he's soloing if he were 
under 10% health and the mob's over 90%, because, say, he's a lowbie 
rogue.  The rule then expressly tells him to go save that soloing necro 
over there, doesn't it?  (This one might get a little less likely with the 
Kant version, if we assume the lowbie rogue's ever played other classes.)

But the ultimate point is, the Golden Rule there (ether version) doesn't 
incorporate any notion of people having differing desires in regards to 
how they are treated, and given the great diversity in how people wish to 
be treated, it might be worth considering an amendment.  It's still a 
pretty darn good rule, though, I won't argue that.  I also won't argue 
that Kant's version is an improvement over the original, but perhaps not 
quite the right improvement.

[65 Coercer] Zinphandel Chianti <Prism> (Gnome) Ayonae Ro


